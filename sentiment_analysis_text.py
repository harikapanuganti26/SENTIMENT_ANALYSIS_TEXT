# -*- coding: utf-8 -*-
"""SENTIMENT_ANALYSIS_TEXT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QTjIjgqr6rF6VNnfepQ49uhCLVxso5LV

#EMOTION DETECTION OF TEXT

+ Text Classification
+ Sentiment Analysis
"""

#Load packages 
import pandas as pd #Used to read csv dataset file and perform functions on Data frames
import numpy as np

#Load Data Viz pkgs to plot bar graphs 
import matplotlib.pyplot as plt
import seaborn as sns

#Text cleaning
!pip install neattext #NLP package for cleaning data (removing unwanted data from data set)
import neattext.functions as nfx

#Load Dataset
df=pd.read_csv("/content/sample_data/Working_dataset.csv") #Reading Dataset 'Working_dataset.csv' file

df.head()

#Shape of Databse
df.shape #Total number of rows and coloumns

#Datatypes in Dataset
df.dtypes

#Check for any Missing Values 
df.isnull().sum() #To check if any coloumn or row is nill

#Value counts of emotions
df['Emotion'].value_counts() #Gives number count of each emotion in Dataset

#Value counts of emotions
df['Emotion'].value_counts().plot(kind='bar') #PLotting the value counts

#Using seaborn to plot value count of emotions
plt.figure(figsize=(20,10))
sns.countplot(x='Emotion',data=df)
plt.show()

"""###Exploration

+ Text Cleaning
+ Sentiment Analysis
+ Keyword Extraction
  - Key words (features) extraction for each emotion
  - Display using Wordcloud

"""

#Sentiment Analysis
from textblob import TextBlob

def get_sentiment(text):
  blob=TextBlob(text)
  sentiment = blob.sentiment.polarity
  if sentiment>0:
    result="Positive"
  elif sentiment<0:
    result="Negative"
  else:    
    result="Neutral"
  return(result)

#Test Function
get_sentiment("I hate running") #Calling the function

#Applying it to the enitre Dataset
df['Sentiment'] = df['Text'].apply(get_sentiment) #Creating a new data frame 'Sentiment' and applying 'get_sentiment()' function to the text and assigning it's value

df.head() #Preview the changes

#Compare our Emotion vs Sentiment
df.groupby(['Emotion','Sentiment']).size() #Grouping Emotion and Sentiment and checking total number of value counts in Data Set

#PLotting the above using Matplotlib
#Emotion vs Sentiment
df.groupby(['Emotion','Sentiment']).size().plot(kind='bar')

#Plotting Emotion vs Sentiment using seaborn for Asthetics and Simple understanding 
#Used to draw different kinds of plot
sns.factorplot
sns.catplot

sns.catplot(x='Emotion',hue='Sentiment',data=df,kind='count',height=6,aspect=1.5) #Assigning values and size of plot

"""###Text Cleaning 
+ Remove Noise
 - Stopwords 
 - Special characters
 - Punctuations 
 - Emojis
"""

dir(nfx) #Returns properties of (nfx) neattext functions that cleans data by removing noise

df['Clean_Text']=df['Text'].apply(nfx.remove_stopwords) #Removing stopwords like ("a","the","is","are") from Text and assigning it to new data frame called Clean_Text

df['Clean_Text']=df['Clean_Text'].apply(nfx.remove_punctuations) #Removing punctations like ("!","?",".") from Text and assigning it to new data frame called Clean_Text

df['Clean_Text']=df['Clean_Text'].apply(nfx.remove_userhandles) #Removing user_handles like @Imposter123

df[['Text','Clean_Text']] #Displaying the Clean_Text

"""###Key word Extraction
+ Extract common words per class of emoiton
"""

from collections import Counter #Keep the count of values

#Defining a function to extract keywords
def extract_keywords(text,num=50):
  tokens = [tok for tok in text.split()] #Splitting text into words and storing list in tokens
  most_common_tokens = Counter(tokens).most_common(num)
  return dict(most_common_tokens)#Returns the most common key words from each emotion

#List of Emotions
emotion_list= df['Emotion'].unique().tolist() #Assigning emotion types and forming a list

emotion_list

joy_list=df[df['Emotion']=='joy']['Clean_Text'].tolist()#Combining Emotion 'joy' with Clean_Text and forming a list

#Joy document
joy_docx = ' '.join(joy_list)

joy_docx

#Extract Joy Keywords
keyword_joy = extract_keywords(joy_docx)

keyword_joy

#Plotting the number of times each keyword present
def plot_most_common_words(mydict,emotion_name):
  df_01 = pd.DataFrame(mydict.items(),columns=['token','count'])
  plt.figure(figsize=(20,10))
  plt.title("Plot of {} Most Common Keywords".format(emotion_name))
  sns.barplot(x='token',y='count',data = df_01)
  plt.xticks(rotation=45)
  plt.show()

plot_most_common_words(keyword_joy,"joy")#Plotting most common joy keywords

#Repeating same steps for 'Surprise' emotion
surprise_list=df[df['Emotion']=='surprise']['Clean_Text'].tolist()
#Document
surprise_docx = ' '.join(surprise_list)
#Extract keywords
keyword_surprise = extract_keywords(surprise_docx)

plot_most_common_words(keyword_surprise,"Surprise")

#Wordcloud is visual representation of words
from wordcloud import WordCloud

#Defining a function to display the most common words using wordcloud
def plot_wordcloud(docx):
  mywordcloud = WordCloud().generate(docx)
  plt.figure(figsize=(20,10))
  plt.imshow(mywordcloud,interpolation='bilinear')
  plt.axis('off')
  plt.show()

plot_wordcloud(joy_docx)

"""###Machiene Learning
+ Naive Bayes
+ LogisticRegression

"""

#Load ML Pkgsfrom
from sklearn.linear_model import LogisticRegression #From sklearn extracting Logistic Regression Feature
from sklearn.naive_bayes import MultinomialNB       #From sklearn extracting MultinomialNB Feature

#Vectorizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer #Used to factorize data

#Metrics
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,plot_confusion_matrix #Importing fuctions for accuracy,confusion matrix and plotting

#Split our dataset
from sklearn.model_selection import train_test_split #Splitting into Training and Testing Data

"""###Build Features from our Text"""

Xfeatures = df['Clean_Text'] #Assigning Clean_Text data frame to Xfeatures
ylabels = df['Emotion'] #Assigning Emotion data frame to ylabels

Xfeatures

#Vectorizer
cv = CountVectorizer() #Convert text into a vector
X=cv.fit_transform(Xfeatures) #Meand and variance of each feature

#Get features by Name
cv.get_feature_names()

#Split Dataset
X_train,X_test,y_train,y_test = train_test_split(X,ylabels,test_size=0.3,random_state=42) #Splitting Datasets and assigning them to different variables

"""#Build our Model"""

nv_model = MultinomialNB()
nv_model.fit(X_train,y_train)

#Accuracy 
nv_model.score(X_test,y_test)

#Predictions are made to testing variable
y_pred_for_nv = nv_model.predict(X_test)

y_pred_for_nv

"""###Make Single Prediction
+ Vectorized our Text
+ Applied our model

"""

sample_text = [' I love coding ']

vect = cv.transform(sample_text).toarray() #Applying transform function to sample_text

#Make Predicition
nv_model.predict(vect) #Prediction for sample_text

#Check for prediction Probability(Percentage) of each 'Emotion'
nv_model.predict_proba(vect)

#Get all class for our Model
nv_model.classes_

np.max(nv_model.predict_proba(vect)) #To return maximum percentage value for our sample_text

#Defining a function to return prediction based on previously trained machiene model by passing "User Input" and Machiene model(nv_model)

def predict_emotion(review,model):
    myvect=cv.transform(review).toarray()
    prediction=model.predict(myvect)
    pred_proba=model.predict_proba(myvect)
    pred_percentage_for_all=dict(zip(model.classes_,pred_proba[0]))
    print("Prediction:{} , Prediction Score: {}".format(prediction[0],np.max(pred_proba)))
    return pred_percentage_for_all

s=["worst"]
predict_emotion(s, nv_model)

print(classification_report(y_test,y_pred_for_nv))

confusion_matrix(y_test,y_pred_for_nv)

#Plot confusion matrix
plot_confusion_matrix(nv_model,X_test,y_test)

import joblib
model_file= open("working_sentiment_model.pkl","wb")
joblib.dump(nv_model,model_file)
model_file.close()

#Logistic Regression
lr_model=LogisticRegression()
lr_model = LogisticRegression( max_iter=1000)
lr_model.fit(X_train,y_train)

#Accuracy
lr_model.score(X_test,y_test)

#Single predict
predict_emotion(s,lr_model)